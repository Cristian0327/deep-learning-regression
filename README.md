 Justificaci贸n de la Elecci贸n de los Hiperpar谩metros
En este proyecto, se implement贸 un modelo de regresi贸n lineal con TensorFlow/Keras para predecir la evoluci贸n de un paciente. Los siguientes hiperpar谩metros fueron seleccionados despu茅s de m煤ltiples pruebas y ajustes manuales:

 Optimizer: Adam
Se eligi贸 Adam porque es un optimizador adaptativo que ajusta la tasa de aprendizaje din谩micamente, lo que acelera la convergencia y evita quedar atrapado en m铆nimos locales.
Se prob贸 con SGD, pero el modelo tardaba m谩s en converger y ten铆a mayor error.
 Learning Rate: 0.0005
Un learning rate bajo permite una convergencia estable sin grandes oscilaciones en la p茅rdida.
Se prob贸 con valores m谩s altos (0.01) y el modelo no converg铆a bien.
 Epochs: 300
Se seleccion贸 300 茅pocas porque los gr谩ficos de loss y MAE muestran que el modelo sigue mejorando hasta ese punto.
Con menos 茅pocas (100 o 200), el modelo ten铆a mayor MAE.
 Batch Size: 8
Se usa un batch size de 8 porque es un valor peque帽o que permite una mejor actualizaci贸n de pesos en cada iteraci贸n.
Se prob贸 con 32, pero el entrenamiento no era tan preciso.
 Funci贸n de P茅rdida: Mean Squared Error (MSE)
MSE penaliza m谩s los errores grandes, lo que ayuda a optimizar la predicci贸n del modelo.
 Inicializaci贸n de Pesos: Default (Xavier/Glorot)
Se us贸 la inicializaci贸n por defecto de TensorFlow (Glorot Uniform), ya que mantiene los valores iniciales de los pesos en un rango 贸ptimo.
 Resultados Finales
P茅rdida final en validaci贸n (loss): 0.3017
Error Absoluto Medio (MAE): 0.4492
El modelo logra un bajo error en validaci贸n, lo que indica que generaliza bien.
